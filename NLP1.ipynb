{"cells":[{"cell_type":"markdown","metadata":{"id":"W_OvfaLGjlgd"},"source":["# **Goal: Create a high-quality corpus from diverse and noisy text sources (COVID-related documents).**\n","\n","Key Features:\n","- Web scraping with Selenium and BeautifulSoup\n","- PDF extraction using PDFMiner\n","- Text normalization, cleaning, and tokenization\n","- Lemmatization and stopword removal using NLTK\n","- Unified corpus creation for further NLP tasks\n"]},{"cell_type":"markdown","metadata":{"id":"Dexa2L1bjlge"},"source":["### Environment setup (Google Colab only)\n","After downloading and extracting the handout tar file, upload all of the handout files to Colab. Run the following cell to list all the file names in the current directory. Make sure you can see `domain_data_preparation.ipynb`, `local_test_refs`, `submitter`, `references`, `pdfs` and `requirements.txt`."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"47kFRhFBjlge","outputId":"84445e84-5b6d-4695-9770-ec7cd397f486"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config',\n"," 'local_test_refs',\n"," 'pdfs',\n"," 'requirements.txt',\n"," '.ipynb_checkpoints',\n"," 'sample_data']"]},"metadata":{},"execution_count":3}],"source":["import os\n","os.listdir()"]},{"cell_type":"code","source":["# import sys\n","# print(sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3fR7HQVlbp6","outputId":"0fb71c25-9968-4225-814c-af2d4d576857"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Rw7uNqsljlge"},"source":["Now install all the Python packages needed for the project."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"_JyuyY_2jlgf","outputId":"53764ae3-67d5-4ee6-90c5-55207cde7916"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.21.2 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0rc1, 2.2.0, 2.2.1, 2.2.2, 2.2.3)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.21.2\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"u0LBGsS9jlgf"},"source":["Finally, run the following code to get a Chrome webdriver in the current directory:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"FhJjmmLPjlgf","outputId":"1128c952-b0b7-4a3d-80c1-15b6b495fd5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,649 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,688 kB]\n","Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,321 kB]\n","Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,708 kB]\n","Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,664 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,830 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,956 kB]\n","Fetched 29.0 MB in 8s (3,468 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n","Suggested packages:\n","  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n","The following NEW packages will be installed:\n","  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n","  systemd-hwe-hwdb udev\n","0 upgraded, 9 newly installed, 0 to remove and 32 not upgraded.\n","Need to get 30.1 MB of archives.\n","After this operation, 123 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.66.1+22.04 [27.6 MB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n","Fetched 30.1 MB in 2s (14.0 MB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package apparmor.\n","(Reading database ... 124947 files and directories currently installed.)\n","Preparing to unpack .../0-apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n","Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n","Selecting previously unselected package liblzo2-2:amd64.\n","Preparing to unpack .../1-liblzo2-2_2.10-2build3_amd64.deb ...\n","Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n","Selecting previously unselected package squashfs-tools.\n","Preparing to unpack .../2-squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n","Unpacking squashfs-tools (1:4.5-3build1) ...\n","Selecting previously unselected package udev.\n","Preparing to unpack .../3-udev_249.11-0ubuntu3.12_amd64.deb ...\n","Unpacking udev (249.11-0ubuntu3.12) ...\n","Selecting previously unselected package libfuse3-3:amd64.\n","Preparing to unpack .../4-libfuse3-3_3.10.5-1build1_amd64.deb ...\n","Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n","Selecting previously unselected package snapd.\n","Preparing to unpack .../5-snapd_2.66.1+22.04_amd64.deb ...\n","Unpacking snapd (2.66.1+22.04) ...\n","Setting up apparmor (3.0.4-2ubuntu2.4) ...\n","Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n","Setting up liblzo2-2:amd64 (2.10-2build3) ...\n","Setting up squashfs-tools (1:4.5-3build1) ...\n","Setting up udev (249.11-0ubuntu3.12) ...\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n","Setting up snapd (2.66.1+22.04) ...\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n","Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n","Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n","Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n","Selecting previously unselected package chromium-browser.\n","(Reading database ... 125384 files and directories currently installed.)\n","Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","=> Installing the chromium snap\n","==> Checking connectivity with the snap store\n","===> System doesn't have a working snapd, skipping\n","Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package systemd-hwe-hwdb.\n","Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n","Unpacking systemd-hwe-hwdb (249.11.5) ...\n","Setting up systemd-hwe-hwdb (249.11.5) ...\n","Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Processing triggers for udev (249.11-0ubuntu3.12) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"]}],"source":["!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver ."]},{"cell_type":"markdown","metadata":{"id":"A0psRzuXjlgf"},"source":["Now you can import the packages and begin the project!"]},{"cell_type":"markdown","metadata":{"id":"WYEUDL_yjlgf"},"source":["### Package import"]},{"cell_type":"code","source":["# !pip install selenium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewsFLB_NraGF","outputId":"784156a9-6ba2-4bb2-80d8-e28b968ac651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n","Successfully installed outcome-1.3.0.post0 selenium-4.29.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"]}]},{"cell_type":"code","source":["# !pip install pdfminer.six\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yL6KYrsHrhph","outputId":"41684fb3-4e2e-4575-9ec9-669f1d07c540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdfminer.six\n","  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n","Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pdfminer.six\n","Successfully installed pdfminer.six-20240706\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tbds2wTnjlgf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import requests\n","import json, collections, time, re, string, os\n","from datetime import datetime\n","\n","import nltk\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","import bs4\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","from selenium.common import exceptions\n","\n","from pdfminer import high_level"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"7I72tv1Rjlgg"},"outputs":[],"source":["# this cell has been tagged with excluded_from_script\n","# it will not be run by the autograder\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"bnFMb_lrjlgg"},"source":["## Text Processing"]},{"cell_type":"markdown","metadata":{"id":"MNKPymdsjlgg"},"source":["Text data on the internet is very messy.  Typically there is a fair amount of processing work to do once you have collected any sizeable chunk of text data, in order to have it ready for subsequent analyses. To get you familiar with this kind of data, this section will walk you through some common processing tasks:\n","\n","The first step is to import the lemmatizer and set of English stopwords from `nltk`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3qWL3fcjlgg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df65611c-0d2e-4669-8da8-9c9e546f01e7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}],"source":["nltk.download(\"stopwords\", quiet = True)\n","nltk.download(\"wordnet\", quiet = True)\n","nltk.download(\"punkt\", quiet = True)\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger', quiet = True)\n","nltk.download('averaged_perceptron_tagger_eng', quiet = True)\n","lemmatizer = WordNetLemmatizer()\n","english_stopwords = set(nltk.corpus.stopwords.words('english'))"]},{"cell_type":"markdown","metadata":{"id":"W920oIhsjlgg"},"source":["**Text cleaning and tokenization**\n","Implement the three functions `clean_string`, `tokenize` and `lemmatize` that perform the following text preprocessing tasks:\n","\n","1. `clean_text` should:\n","    * convert the string to lower case.\n","    * remove any instance of `'s` that is either followed by any whitespace character, or at the end of the string: `teacher's help` becomes `teacher help`, and `children's` becomes `children`.\n","    * remove apostrophe character `'`: `don't` becomes `dont`. For simplicity we will only consider the character `'` as apostrophe (so `’` is not).\n","    * remove leading and trailing space.\n","\n","1. `tokenize` should:\n","    * use `nltk.word_tokenize` to tokenize the input text.\n","    * further break tokens at characters which are not digits 0-9 and not present in `string.ascii_letters`. For example, `a_b_c` becomes `['a', 'b', 'c']`.\n","    * maintain the token order as it appears in the original string.\n","\n","1. `lemmatize` should:\n","    * lemmatize each token individually.\n","    * remove tokens that are stopwords or contain fewer than two characters (these two cases should be checked after the lemmatization step).\n","    \n","**Notes**:\n","* When lemmatizing a word, you should also specify the part-of-speech `pos` parameter. This can be obtained by calling `nltk.pos_tag` and using the first returned tag (in case there are multiple possibilities). You can interpret the returned tag as follows:\n","    * If it starts with \"J\", it is an adjective.\n","    * If it starts with \"V\", it is a verb.\n","    * If it starts with \"R\", it is an adverb.\n","    * Otherwise, it is a noun.\n","* `nltk.pos_tag` should be called on each individual token, instead of on the entire tokenized text. For example, if the input string is `\"learning is fun\"`, you should call `nltk.pos_tag([\"learning\"])` to get the part-of-speech of `'learning'`, and input that to the lemmatizer. You may notice that in this case `\"learning\"` is classified as a verb (while it is a noun in the original sentence). However, this is not a problem, since our end goal is to reduce each token to its base form, not to correctly classify its part-of-speech.\n","* If you use the regex character set `\\w`, note that it matches alphanumeric characters **and** the underscore character `_`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSpOGghvjlgg"},"outputs":[],"source":["# import re\n","def clean_text(text):\n","    \"\"\"\n","    Clean the input string by converting it to lowercase, removing 's and apostrophe.\n","\n","    args:\n","        text (str) : the input text\n","\n","    return:\n","        str : the cleaned text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"'s(\\s|$)\", \" \", text)\n","    text = re.sub(r\"'\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","    pass\n","\n","def tokenize(cleaned_text):\n","    \"\"\"\n","    Tokenize the input string.\n","\n","    args:\n","        cleaned_text (str): the input text, output from clean_text\n","\n","    return:\n","        List[str] : a list of tokens from the input text\n","    \"\"\"\n","\n","    tokens = nltk.word_tokenize(cleaned_text)\n","    split_tokens = []\n","    for token in tokens:\n","        split_tokens.extend(re.findall(r\"[a-zA-Z0-9]+\", token))\n","    return split_tokens\n","\n","def lemmatize(tokens, stopwords = {}):\n","    \"\"\"\n","    Lemmatize each token in an input list of tokens\n","\n","    args:\n","        tokens (List[str]) : a list of token, output from tokenize\n","\n","    kwargs:\n","        stopwords (Set[str]) : the set of stopwords to exclude\n","\n","    return:\n","        List[str] : a list of lemmatized and filtered tokens\n","    \"\"\"\n","    def get_wordnet_pos(word):\n","        tag = nltk.pos_tag([word])[0][1][0].upper()\n","        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n","        return tag_dict.get(tag, wordnet.NOUN)\n","\n","    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n","    lemmatized_tokens = [token for token in lemmatized_tokens if token not in stopwords and len(token) >= 2]\n","    return lemmatized_tokens\n","\n","def preprocess_text(text, stopwords = {}):\n","    # do not modify this function\n","    cleaned_text = clean_text(text)\n","    tokens = tokenize(cleaned_text)\n","    return lemmatize(tokens, stopwords)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"w3Y-WKPxjlgg","outputId":"dff7399b-f893-47e4-a2f5-87b91490a402"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_preprocess_text():\n","    # cleaning\n","    assert clean_text(\"I like Data Science\") == \"i like data science\"\n","    assert clean_text(\"She's\") == \"she\"\n","    assert clean_text(\"you've\")== \"youve\"\n","    assert clean_text(\"car, cars, car's cars'\")== \"car, cars, car cars\"\n","    assert clean_text(\"'shed'\") == \"shed\"\n","    assert clean_text(\"'good news'\") == \"good news\"\n","    assert clean_text(\"CMU's campus\")== \"cmu campus\"\n","    assert preprocess_text(\"abc 'system\") == ['abc', 'system']\n","\n","    assert preprocess_text(\"O'Shea Jackson Jr. is an American actor and musician\") == ['oshea', 'jackson', 'jr', 'be', 'an', 'american', 'actor', 'and', 'musician']\n","    # tokenization\n","    assert tokenize(\"ab..ab. .ab . ab.\") == ['ab', 'ab', 'ab', 'ab'], tokenize(\"ab..ab. .ab . ab.\")\n","    assert tokenize(\"word-of-mouth hello,world\")== ['word', 'of', 'mouth', 'hello', 'world']\n","    assert tokenize(\"gotta\")== ['got', 'ta']\n","    assert tokenize(\"hello_world\") == [\"hello\", \"world\"]\n","    assert preprocess_text(\"hope this👏will work\") == ['hope', 'this', 'will', 'work']\n","\n","    # lemmatization\n","    assert lemmatize([\"cats\"]) == ['cat']\n","    assert lemmatize([\"did\"]) == ['do']\n","    assert lemmatize([\"learning\", \"is\", \"fun\"], english_stopwords) == [\"learn\", \"fun\"]\n","\n","    # miscellaneous\n","    assert preprocess_text(\"the weather is really nice\", english_stopwords) == ['weather', 'really', 'nice']\n","    assert preprocess_text(\n","        \"To apply SVM learning in partial discharge classification, data input is very important!?\",\n","        english_stopwords\n","    ) == 'apply svm learn partial discharge classification data input important'.split()\n","    assert preprocess_text(\"after all he's done\", english_stopwords) == []\n","    assert preprocess_text(\"they didn’t have much chance of guessing what it was without further clues.\", english_stopwords) == ['much', 'chance', 'guess', 'without', 'far', 'clue']\n","    assert preprocess_text(\"DUQUE'S\", english_stopwords) == [\"duque\"]\n","    assert preprocess_text(\"the 'rona\", english_stopwords) == ['rona']\n","    assert preprocess_text('MOTORCYCLES DONT FLY', english_stopwords)==['motorcycle', 'dont', 'fly']\n","    assert preprocess_text('“ Georg e\\”', english_stopwords) == ['georg']\n","    text = \"Harry leapt into the air; he’d trodden on something big and squashy on the doormat — something alive\"\n","    assert preprocess_text(text, english_stopwords) == ['harry', 'leapt', 'air', 'trodden', 'something', 'big', 'squashy', 'doormat', 'something', 'alive']\n","    assert preprocess_text(\"Donâ€™t want to add to TRUMPâ€™s #COVID19 numbers. #CoronaVirus ðŸ¦  donâ€™t care.\", english_stopwords) == ['want', 'add', 'trump', 'covid19', 'number', 'coronavirus', 'care']\n","\n","    # test on long text string\n","    with open(\"local_test_refs/henrys_letter.txt\", encoding = \"utf-8\") as infile, open(\"local_test_refs/processed_henrys_letter.txt\", encoding = \"utf-8\") as outfile:\n","        processed_str = preprocess_text(infile.read())\n","        reference_str = outfile.read().splitlines()\n","        assert processed_str == reference_str\n","    print(\"All tests passed!\")\n","\n","test_preprocess_text()"]},{"cell_type":"markdown","metadata":{"id":"jHPO-xbRjlgg"},"source":["You may notice that the lemmatization functionality isn't perfect; for example, it would map `\"as\"` to `\"a\"` because `\"as\"` is being treated as a noun instead of a proposition (with tag `\"IN\"`). In general, identifying the correct part-of-speech tag is very context-dependent (for example, `\"back\"` can be either an adjvective, adverb, verb or noun). In the context of this project, we will not dive deep into these linguistic nuances, and settle with the lemmatization rules above.\n","\n","The above processing function already covers a fair number of text cleaning tasks. We can now begin to collect data from online sources."]},{"cell_type":"markdown","metadata":{"id":"0HZrj97Ejlgg"},"source":["## Part B: Tweet Mining"]},{"cell_type":"markdown","metadata":{"id":"IcOnjDSQjlgg"},"source":["Twitter is one of the most popular social media platforms; according to [Omnicore](https://www.omnicoreagency.com/twitter-statistics/#:~:text=There%20are%2048.35%20million%20monthly,monetizable%20daily%20active%20Twitter%20users.), it has 48.35 million active users, 42% of whom use Twitter on a daily basis. Furthermore, tweets are public by default, making the site a particularly rich data source on any given trending topic.\n","\n","In this section, you will extract tweets related to the topic of coronavirus. Due to the dynamic nature of Twitter, any user can edit or remove their old tweets, making it difficult to obtain deterministic results (and to autograde your code). Therefore, we will instead use a fixed Tweet [dataset from Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets) and provide you with a custom API to query tweets from this dataset. For your own data science projects in the future, however, you are encouraged to explore the [official Twitter API](https://developer.twitter.com/en/docs)."]},{"cell_type":"markdown","metadata":{"id":"SLz8xcNBjlgg"},"source":["###  Retrieve starting tweets\n","Implement the function `get_tweets` that sends a GET request to https://gettweets.azurewebsites.net/11637/tweets and returns the status code as well as the response JSON. The response JSON is a list of dictionaries, each corresponding to one tweet and having the following format:\n","\n","```python\n","{\n","    'text': 'hello world', # str, the tweet content\n","    'lang': 'en', # str, the tweet language\n","    'id': 123, # int, the tweet id\n","    'time': '2019-12-04' # # str, yyyy-mm-dd\n","}\n","```\n","\n","**Notes**:\n","* The API endpoint url https://gettweets.azurewebsites.net/11637/tweets is provided in the global variable `TWEET_API`.\n","* You should call `.text` on the response object to retrieve its content, and then convert the content to JSON before returning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7A3LNKRjlgg"},"outputs":[],"source":["TWEET_API = \"https://gettweets.azurewebsites.net/11637/tweets\"\n","\n","def get_tweets(url = TWEET_API):\n","    \"\"\"\n","    Retrieve tweets by sending a GET request to the provided API endpoint.\n","\n","    params:\n","        url (str) : the url to send request to\n","\n","    return:\n","        Tuple(status_code, response):\n","            status_code (int) : the response status code\n","            response (str) : the response text\n","    \"\"\"\n","\n","\n","    response = requests.get(url)\n","\n","    if response.status_code == 200:\n","        tweets = response.json()\n","\n","        sorted_tweets = [\n","            {\"text\": tweet[\"text\"], \"lang\": tweet[\"lang\"], \"id\": tweet[\"id\"], \"time\": tweet[\"time\"]}\n","            for tweet in tweets\n","        ]\n","\n","        return response.status_code, json.dumps(sorted_tweets)\n","\n","    return response.status_code, \"[]\"\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"-0TqpHDqjlgh","outputId":"959cb7aa-7ee1-4a4f-d975-2d2a6fd5b963"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_get_tweets():\n","    response_code, tweet_text = get_tweets()\n","    tweet_jsons = json.loads(tweet_text)\n","    assert response_code == 200\n","    assert len(tweet_jsons) == 100\n","    first10_tweet_ids = [tweet_json[\"id\"] for tweet_json in tweet_jsons[:10]]\n","    assert first10_tweet_ids == [2819, 3075, 3331, 3587, 3843, 4099, 4355, 4611, 4867, 5123]\n","    assert tweet_jsons[1][\"text\"] == 'Los Angeles County has identified 6 new cases of the coronavirus &amp; declared a local state of emergency.\\n#CoronavirusOutbreak #Coronavirusflorida #LosAngeles'\n","    print(\"All tests passed!\")\n","\n","test_get_tweets()"]},{"cell_type":"markdown","metadata":{"id":"vdIHncVkjlgh"},"source":["Great, you now have 100 tweets at your disposal! Typically though, we would like to have more flexibility in our search, for example by specifying particular parameters that indicate our search objective. In this case, our API provides four parameters as follows:\n","\n","* `lang` - specify the tweet language, which is an [ISO 639-1 code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes), e.g., `en` for English.\n","* `start` - the start date, formatted as yyyy-mm-dd; only tweets created on or after this date are returned.\n","* `end` - the end date, formatted as yyyy-mm-dd; only tweets created on or before this date are returned.\n","* `page` - the tweet page number."]},{"cell_type":"markdown","metadata":{"id":"K255m6KFjlgh"},"source":["Note the use of the `page` parameter here. Because returning a large JSON would put burden on the server, we have restricted all response JSONs to only include 100 tweets. To get more tweets that also satisfy your search query, you can specify the `page` parameter based on the following formula: if `page = i` (indexed from 1) then the tweets from index `100*(i-1) + 1` to index `100*i` (inclusive) will be returned.\n","\n","As an example, if your configuration of `(lang, start, end)` yields 306 tweets, then:\n","\n","* `page = 1` or no `page` specified will return the tweets 1-100.\n","* `page = 2` will return the tweets 101-200.\n","* `page = 3` will return the tweets 201-300.\n","* `page = 4` will return the tweets 301-306.\n","* `page = 5` or larger will return a JSON with empty content `\"[]\"`."]},{"cell_type":"markdown","metadata":{"id":"fdmaH2Bxjlgh"},"source":["###  Search for tweets with parameters\n","Implement the function `get_tweet_texts_with_params` that sends a GET request to the provided API endpoint with (optional) input parameters `lang`, `start`, `end` and `page`. This function should collect all tweets that satisfy the search query if the number of such tweets is smaller than a specified `n_tweets`, or the first `n_tweets` tweets otherwise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnMc6dSijlgh"},"outputs":[],"source":["def get_tweet_texts_with_params(url = TWEET_API, lang = \"all\", start = \"na\", end = \"na\", n_tweets = 10000):\n","    \"\"\"\n","    Search for tweets with parameters and extract their text content\n","\n","    kwargs:\n","        url (str) : the url to send request to\n","        lang (str) : the tweet language in ISO 639-1 format\n","        start (str) : the start date, yyyy-mm-dd\n","        end (str) : the end date, yyyy-mm-dd\n","        n_tweets (int) : the number of tweets to collect\n","\n","    return:\n","        List[str] : a list with the contents of the first n_tweets tweets returned from the search\n","    \"\"\"\n","    collected_tweets = []\n","    page = 1\n","\n","    while len(collected_tweets) < n_tweets:\n","        params = {\"lang\": lang, \"start\": start, \"end\": end, \"page\": page}\n","        response = requests.get(url, params=params)\n","        tweets = response.json()\n","\n","        if not tweets:\n","            break\n","\n","        collected_tweets.extend([tweet[\"text\"] for tweet in tweets])\n","        page += 1\n","\n","    return collected_tweets[:n_tweets]\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"pVAHO-XMjlgh","outputId":"2670a0a0-b12e-4824-e51d-3ba3e25f6129"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_get_tweet_texts_with_params():\n","    tweet_texts = get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\")\n","    assert len(tweet_texts) == 10000\n","    example_tweets = [\n","        '#coronavirus #COVID19 2 new cases reported in Kentucky (Harrison Co and Fayette Co), bring the states total to 6.',\n","        \"#BREAKING\\n#Italy is extending its #coronavirus #quarantine measures, which include a ban on public gatherings, to the ENTIRE COUNTRY.\\nItaly's coronavirus death toll jumped on Monday by 97 to 463. \\nIt is the worst-hit country after #China.\\nhttps://t.co/Q6MNtyXttA\\n#COVID19\",\n","        '#Investorsâ€™ fortunes plunge as sell pressure hit banking stocks amid COVID-19 fear https://t.co/stihfvM7xZ via @MarketForcesA\\n\\n@Afrinvest @nsenigeria #equitymarket #fmcg #banks #insurance #oilandgas #COVID19',\n","        'FROM A DOCTOR IN ITALY...\\n#coronavirus\\n#Coronavirusflorida\\n#CoronaVirusUpdates\\n#CoronavirusUSA\\n#COVID19 \\n#covid19Canada\\n#COVID19Toronto\\n#CoronaVirusCanada \\n#coronavirusToronto https://t.co/ZelZHWTuR1',\n","        'Ina joint statement, Major League Baseball, Major League Soccer, the National Basketball Association, and the National Hockey League announced they are limiting locker room access due to concerns about the Coronavirus pandemic. #MLB #MLS #NBA #NHL #COVID19 #coronavirus https://t.co/Z6CUqxDrAO',\n","        'Coronaviruses (CoV) are a large family of viruses that cause illness ranging from the common cold to more severe diseases such asMERS-CoVand SARS-CoV. A novel coronavirus (nCoV) is a new strain that has not been previously identified in humans.  #coronavirus #CoronavirusOutbreak',\n","        'Simon Coveny and the entire Irish government attitude towards #covid19 just got destroyed on  #cblive',\n","        'The reason for the toilet paper shortage is because when one person sneezes, 100 people shit themselves ðŸ¤§ #coronavirus #COVID19 #tolietpaper',\n","        'Latest #COVID19 numbers in Missouri - https://t.co/27vLQ3ZmqY',\n","        'Tips on battling #CoronaVirusUpdate #COVID2019 #CoronavirusOutbreak \\nhttps://t.co/UaY8boPzHg'\n","    ]\n","\n","    assert tweet_texts[800:810] == example_tweets\n","    emoji_tweet = \"Be carefull guy's and wish you all happy holi to you &amp; your family. :) \\n#HappyHoli #CoronavirusOutbreak #à¤¹à¥‹à¤²à¥€ #à¤¹à¥‹à¤²à¤¿à¤•à¤¾_à¤¦à¤¹à¤¨ #BankLooteriBJP #Coronavid19 #marketcrash  #reliance #colours #KurkureWithSidNaaz #MondayMorning #MereAngneMein ##RangBarseWithSid #à¤¬à¥�à¤°à¤¾_à¤¨_à¤®à¤¾à¤¨à¥‹_à¤¹à¥‹à¤²à¥€_à¤¹à¥ˆ https://t.co/Rg2SpMNKZD\"\n","    assert emoji_tweet in tweet_texts\n","    print(\"All tests passed!\")\n","\n","test_get_tweet_texts_with_params()"]},{"cell_type":"markdown","metadata":{"id":"2GU7Lqgjjlgh"},"source":["###  Process tweet data\n","Looking at some of the tweets above, we see that:\n","1. Some tweets contain Twitter-shortened URLs, for example `https://t.co/DzhsXPxUDa`. These are always in the form of `http://t.co/` or `https://t.co/` followed by 10 alphanumeric characters. These links should be removed.\n","1. Some tweets contain emoticons such as `:)` or `<3`. The characters in these emoticons should be removed.\n","\n","Implement the function `process_tweet` that takes as input a tweet text, performs the above two cleaning steps, and then calls `preprocess_text` on the cleaned tweet.\n","\n","**Notes**:\n","* You should remove URL before removing emoticons.\n","* We have provided a list of emoticons for you in the variable `emoticons`. You can assume that only elements in this set are considered emoticons and need to be removed.\n","* Note that there may be no space between a shortened URL and the next word. However, you can assume that there are always 10 alphanumeric characters after http://t.co/ or https://t.co/.\n","* Remember to specify the stopwords parameter `english_stopwords` when calling `preprocess_text`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnbjXWZqjlgh"},"outputs":[],"source":["emoticons = [\n","    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n","    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n","    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n","    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3',\n","    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n","    ':-[', ':-<', '=\\\\', 'b=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n","    ':c', ':{', '>:\\\\', ';('\n","]\n","\n","\n","def process_tweet(tweet_text):\n","    \"\"\"\n","    Process and tokenize tweets, in addition to removing URLs and emoticons\n","\n","    args:\n","        tweet_text (str) : a list of tweet contents\n","\n","    return:\n","        List[str] :  a list of processed tokens from the input tweet\n","    \"\"\"\n","    tweet = re.sub(r\"https?://t\\.co/\\w{10}\", \" \", tweet_text)\n","\n","    for emoticon in emoticons:\n","        tweet = tweet.replace(emoticon, \"\")\n","\n","    tweet = \" \".join(tweet.split())\n","\n","    return preprocess_text(tweet, english_stopwords)\n","\n","def process_tweet_data(tweet_texts):\n","    return [process_tweet(tweet_text) for tweet_text in tweet_texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/"},"id":"7pBZxmCjjlgh","outputId":"1156617d-4694-4ef2-bbe4-9c7d034e8c75"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_process_tweet():\n","    assert process_tweet(\"It's a great day :D\") == ['great', 'day']\n","    assert process_tweet(\"<3hello\") == [\"hello\"]\n","    # assert process_tweet(\"goodX-Dday\") == [\"good\", \"day\"]\n","    assert process_tweet(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\") == []\n","    assert process_tweet(\"hellohttp://t.co/WJs5bmRthUworld\") == [\"hello\", \"world\"]\n","    assert process_tweet(\"http://taco/WJs5bmRthU\") == ['http', 'taco', 'wjs5bmrthu']\n","    assert process_tweet(\n","        'Protect your child from #CoronavirusOutbreak.\\n\\nhttps://t.co/qPREVvM2C5\\n\\n#CoronaVirusUpdate #COVID2019 #COVID #Coronavid19 #outbreak #Italy #COVIDãƒ¼19 #BeSafe #Containment #Homeschooling #DigitalTransformation #InternationalSchooling #virtualschool #OnlineNOW #edtech #technology'\n","    ) == ['protect', 'child', 'coronavirusoutbreak', 'coronavirusupdate', 'covid2019', 'covid', 'coronavid19', 'outbreak', 'italy', 'covid', '19', 'besafe', 'containment', 'homeschooling', 'digitaltransformation', 'internationalschooling', 'virtualschool', 'onlinenow', 'edtech', 'technology']\n","    print(\"All tests passed!\")\n","\n","test_process_tweet()"]},{"cell_type":"markdown","metadata":{"id":"knapKXINjlgh"},"source":["##Web Scraping"]},{"cell_type":"markdown","metadata":{"id":"E0WWHRJHjlgh"},"source":["We now move to the second method of data extraction: using Selenium and Beautifulsoup to parse HTML codes. More specifically, we will collect news articles related to the same topic of Coronavirus from two major media outlets -- [Nature](https://www.nature.com/) and [The New York Times](https://www.nytimes.com/). Through this exercise, you will learn how to navigate HTML structures from different webpages in order to get the desired information.\n","\n","To begin, we have provided you a helper function `retrieve_url` takes as input a webpage string URL and creates a BeautifulSoup object from the corresponding page content."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QbmSZrFjlgh"},"outputs":[],"source":["def retrieve_url(url):\n","    page = requests.get(url)\n","    soup = BeautifulSoup(page.content, 'html.parser')\n","    return soup"]},{"cell_type":"markdown","metadata":{"id":"cNZNdfWdjlgh"},"source":["### Parsing a single article from Nature\n","Implement the function `parse_page_nature` that takes as input a URL string pointing to a Nature news article, and returns a JSON dictionary with the following format:\n","\n","```python\n","{\n","    'Title': 'When will the coronavirus outbreak peak?' #str\n","    'Author': ['David Cyranoski'] # list, a list of author names in the same order as they appear on the page\n","    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n","    'Summary': '.....' #str, the summary div between the title and author fields, or empty string if no summary is available\n","    'Content': '.....' #list, the whole article content, where every element is a paragraph (i.e., comes from a <p> tag)\n","}\n","```\n","\n","The values of `Summary` and `Content` should be raw texts that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b><a href=\"https://google.com\">World</a><p>\"` then the output `Content` should be `\"Hello World\"`.\n","\n","In the local test we have provided the full reference JSON files for some article pages. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n","\n","**Notes**:\n","* Occasionally there are some \"Related\" blocks embedded in the article text (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nature_related.png)). These are characterized by the attribute `data-label=\"Related\"` and should **not** be included in the parsing result.\n","* The `Published Date` field should be the original article date, not the updated date. For example, the `Published Date` for [this article](http://web.archive.org/web/20210308142952/https://www.nature.com/articles/d41586-020-00166-6) is 2020-01-22.\n","* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A.\n","* Do not parse information form the `meta` tags as they are not robust. Every required information can be found within `body`.\n","* If an article has no authors (e.g., [this article](http://web.archive.org/web/20201116084933/https://www.nature.com/articles/d41586-020-00589-1)), the Author field should be an empty list.\n","* For the Content list, only text contents that come from the `p` tags in the article body should be included. You can start by identifying a `div` that corresponds to the entire article body (looking at the CSS class names may be helpful). Note that if an image caption is the child of a `p` tag, its content should be included as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36tnDRcwjlgi"},"outputs":[],"source":["def parse_page_nature(url):\n","    \"\"\"\n","    Parse a single New York Times article at the given URL\n","\n","    args:\n","        url (str) : the article URL\n","\n","    return:\n","        Dict[str, str] : the parsed information stored in JSON format, which includes:\n","            Title, Author, Published Date, Summary and Content\n","    \"\"\"\n","\n","    soup = retrieve_url(url)\n","\n","    title = soup.find('title')\n","    title_text = title.text.strip() if title else print(\"issue\")\n","\n","    authors = [author.get_text(strip=True) for author in soup.select(\"h3#author-affiliation-news-0-head + a span.block.hide-overflow.nowrap.overflow-ellipsis\")]\n","\n","    date = soup.find('time', {'itemprop': 'datePublished'})\n","    published_date = date.get_text(strip=True) if date else \"\"\n","\n","\n","    if published_date:\n","        try:\n","            from datetime import datetime\n","            published_date = datetime.strptime(published_date, \"%d %B %Y\").strftime(\"%Y-%m-%d\")\n","        except ValueError:\n","            pass\n","\n","    summary = soup.find('div', class_='article-item__teaser-text')\n","    summary_text = summary.get_text(strip=True) if summary else \"\"\n","\n","    content_div = soup.find('div', class_='c-article-body') or soup.find('div', class_='article__body')\n","    content = []\n","    if content_div:\n","        for p in content_div.find_all('p'):\n","            if not p.has_attr('data-label') or p['data-label'] != 'Related':\n","                text = p.get_text(\" \", strip=True)\n","                if text:\n","                    content.append(text)\n","\n","    return {\n","        \"Title\": title_text,\n","        \"Author\": authors,\n","        \"Published Date\": published_date,\n","        \"Summary\": summary_text,\n","        \"Content\": content\n","    }\n","\n","\n","\n","\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"colab":{"base_uri":"https://localhost:8080/","height":555},"id":"dlMRA-ZPjlgi","outputId":"2e93ecb7-eb96-495c-e1f4-aa47d09e1aee"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"{'Title': 'This scientist hopes to test coronavirus drugs on animals in locked-down Wuhan', 'Author': [], 'Published Date': '2020-01-24', 'Summary': 'Structural biologist Rolf Hilgenfeld has been working on coronavirus treatments since the SARS outbreak.', 'Content': ['Coronaviruses take their name from their crown-like halo. Credit: EYE OF SCIENCE/SPL', 'Shanghai, China', 'As China battles to halt the spread of a new virus that has infected hundreds of people, one structural biologist is hoping to get to the epicentre of the outbreak, the locked-down city of Wuhan.', 'Rolf Hilgenfeld, who is based at the University of Lübeck in Germany, has been trying to develop a cure for coronaviruses since the 2002–03 outbreak of severe acute respiratory syndrome (SARS). Hilgenfeld is hoping to get into Wuhan in central China, to work with researchers there to test two compounds in animals infected with the new, related coronavirus, which emerged in the city late last year. The early-stage drug candidates are not ready for use in people, but Hilgenfeld wants to start animal testing with the aim of developing treatments for future coronavirus outbreaks.', 'Structural biologist Rolf Hilgenfeld. Credit: University of Lübeck', 'Hilgenfeld, who is currently in China, tells Nature about his quest — and the obstacles.', 'I planned to go to China anyway, but after this virus emerged I contacted some collaborators in Wuhan. I have two compounds, and I would like to test them against the new virus, so I am seeking collaborators who have samples of the virus.', 'I had the same experience going to Beijing in 2003 for SARS. I was on a flight with eight people. I had to connect through Japan because there were no flights from Europe. In Wuhan, I will wear a face mask all the time.', 'We have just been getting them ready to be tested in a mouse model of Middle East respiratory syndrome (MERS) with a collaborator in Germany. Those tests should start in the next two weeks. In cell culture, we know they work against SARS and MERS [which are both also caused by coronaviruses]. The compounds have been tested in mice; we know they are safe. But these are not drugs. They are not very advanced. They’ve never been tested in humans.', 'The problem with these antiviral compounds is that when you have the compound ready there are no patients. The new coronavirus outbreak will probably be over in six months, like the SARS one was. After six months, we could have data that show one of our compounds works against the new virus, and would be able to collaborate on developing it as a drug. But if by then the outbreak is over, there will be no patients, so how can you do clinical trials?', 'And the total number of people infected, if you combine SARS, MERS and this new virus, is under 12,500 people. That’s not a market. The number of cases is too small. Pharmaceutical companies are not interested.', 'We have actually developed compounds that are active against both the coronaviruses and a large family of enteroviruses, which include human rhinoviruses [the main cause of common colds] and hand, foot and mouth disease. Every year, half a million children get one called enterovirus-71, so we would aim to go into clinical trials for these diseases. We can get pharma involved. Then if we have something approved for those, we can use the drug quickly next time there is a coronavirus outbreak.', 'They are directed at viral proteases, which have common features in both coronaviruses and enteroviruses. We are structural biologists. We look at the crystal structure and then design something that targets both. It’s killing two birds with one stone, as you say in English, or killing two flies with one swatter, as we say in German.', 'The ones I brought over are a second generation of compounds that we prepared for the mouse MERS experiments. There is a publication about them under review.', 'It’s not so easy to make a mouse model. The mouse doesn’t interact with the MERS or SARS viruses because of a difference in the ACE2 receptor [which the virus uses to enter cells]. You must first engineer a mouse that carries the human version of ACE2 and you must delete the mouse version.', 'If Wuhan airport doesn’t open by next week, I will send my compounds by courier.', 'What happened is that a radio station contacted me because I do work related to SARS. I told them I was going to China and they wrote a story with a question mark in the title asking whether I could save people from the Wuhan virus. That was picked up. The idea that I have a drug is premature. I am trying to correct that.']}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-edb5e19335f7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_parse_page_nature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-edb5e19335f7>\u001b[0m in \u001b[0;36mtest_parse_page_nature\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnature1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_page_nature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://web.archive.org/web/20200430055159/https://www.nature.com/articles/d41586-020-00190-6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnature1_reference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local_test_refs/nature1.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnature1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnature1_reference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnature1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnature2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_page_nature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://web.archive.org/web/20210308142952/https://www.nature.com/articles/d41586-020-00166-6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: {'Title': 'This scientist hopes to test coronavirus drugs on animals in locked-down Wuhan', 'Author': [], 'Published Date': '2020-01-24', 'Summary': 'Structural biologist Rolf Hilgenfeld has been working on coronavirus treatments since the SARS outbreak.', 'Content': ['Coronaviruses take their name from their crown-like halo. Credit: EYE OF SCIENCE/SPL', 'Shanghai, China', 'As China battles to halt the spread of a new virus that has infected hundreds of people, one structural biologist is hoping to get to the epicentre of the outbreak, the locked-down city of Wuhan.', 'Rolf Hilgenfeld, who is based at the University of Lübeck in Germany, has been trying to develop a cure for coronaviruses since the 2002–03 outbreak of severe acute respiratory syndrome (SARS). Hilgenfeld is hoping to get into Wuhan in central China, to work with researchers there to test two compounds in animals infected with the new, related coronavirus, which emerged in the city late last year. The early-stage drug candidates are not ready for use in people, but Hilgenfeld wants to start animal testing with the aim of developing treatments for future coronavirus outbreaks.', 'Structural biologist Rolf Hilgenfeld. Credit: University of Lübeck', 'Hilgenfeld, who is currently in China, tells Nature about his quest — and the obstacles.', 'I planned to go to China anyway, but after this virus emerged I contacted some collaborators in Wuhan. I have two compounds, and I would like to t..."]}],"source":["def test_parse_page_nature():\n","    nature1 = parse_page_nature(\"http://web.archive.org/web/20200430055159/https://www.nature.com/articles/d41586-020-00190-6\")\n","    nature1_reference = json.load(open(\"local_test_refs/nature1.txt\"))\n","    assert nature1 == nature1_reference, nature1\n","\n","    nature2 = parse_page_nature(\"http://web.archive.org/web/20210308142952/https://www.nature.com/articles/d41586-020-00166-6\")\n","    nature2_reference = json.load(open(\"local_test_refs/nature2.txt\"))\n","    assert nature2 == nature2_reference, nature2\n","\n","    nature3 = parse_page_nature(\"http://web.archive.org/web/20200604083109/https://www.nature.com/articles/d41586-020-00798-8\")\n","    nature3_reference = json.load(open(\"local_test_refs/nature3.txt\"))\n","    assert nature3 == nature3_reference, nature3\n","\n","    nature4 = parse_page_nature(\"http://web.archive.org/web/20201116084933/https://www.nature.com/articles/d41586-020-00589-1\")\n","    assert nature4[\"Author\"] == []\n","    assert nature4[\"Published Date\"] == '2020-03-04'\n","    assert len(nature4[\"Content\"]) == 26\n","\n","    nature5 = parse_page_nature(\"http://web.archive.org/web/20200604071303/https://www.nature.com/articles/d41586-020-00786-y\")\n","    assert nature5[\"Author\"] == ['Giuliana Viglione']\n","    assert nature5[\"Published Date\"] == '2020-03-16'\n","    assert len(nature5[\"Content\"]) == 14\n","    print(\"All tests passed!\")\n","\n","test_parse_page_nature()"]},{"cell_type":"markdown","metadata":{"id":"MoTcEFE1jlgi"},"source":["### Parsing several Nature articles from a search page\n","Now that you have successfully parsed individual article pages, the next step is to search for all the relevant articles and collect their titles. More specifically, we want to search for articles that:\n","1. contain the term \"coronavirus\" (case-insensitive) *in their titles*\n","1. do not contain any of the terms \"Daily briefing\", \"Podcast\" or \"Backchat\" (case-insensitive) in their titles\n","1. were published in a given period of time (e.g., February 01, 2020 to March 01, 2020 inclusive)\n","1. have \"News\" as the Article type and belong to the journal \"Nature\" -- these criteria can be specified in the search result page.\n","\n","Explore the [Nature search page](https://www.nature.com/search) and [Advanced search page](https://www.nature.com/search/advanced) to see how you may obtain the desired search results. Then implement the function `extract_nature_articles` that returns a list of titles for articles that meet the search criteria.\n","\n","**Notes**:\n","* You do not need Selenium for this question. Pay attention to how the search parameters are reflected in the URL.\n","* The article titles should be ordered based on their associated dates **on the search result page**, from earlier to later. If two articles have the same date, order them alphabetically based on their titles.\n","* If Nature's search functionalities do not support all of the search criteria, you can implement your own filter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvJQcngfjlgi"},"outputs":[],"source":["def extract_nature_articles(start_date, end_date, base_url = \"https://www.nature.com\"):\n","    \"\"\"\n","    Search for and parse all coronavirus-related News article from the Nature journal that were\n","    published in a given period\n","\n","    args:\n","        start_date (str): the lower bound of the date range to filter articles,\n","            has the format yyyy-mm-dd\n","        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n","            has the format yyyy-mm-dd\n","\n","    kwargs:\n","        base_url (str): the home page url of Nature\n","\n","    return:\n","        List[str] : a list of article titles that meet the search criteria, ordered by\n","            date and by title\n","    \"\"\"\n","    # base_url = \"https://www.nature.com/search\"\n","    # params = {\n","    #     \"q\": \"coronavirus\",\n","    #     \"article_type\": \"news\",\n","    #     \"journal\": \"nature\",\n","    #     \"date_range\": f\"{start_date}%20to%20{end_date}\"\n","    # }\n","\n","    # # Make request to Nature's search page\n","    # response = requests.get(base_url, params=params)\n","    # if response.status_code != 200:\n","    #     raise Exception(f\"Failed to fetch search results. Status Code: {response.status_code}\")\n","\n","    # # Parse the search results page\n","    # soup = BeautifulSoup(response.text, \"html.parser\")\n","    # articles = soup.find_all(\"article\")\n","\n","    # # Extract relevant article titles and dates\n","    # filtered_articles = []\n","    # for article in articles:\n","    #     title_tag = article.find(\"h2\")\n","    #     date_tag = article.find(\"time\")\n","\n","    #     if title_tag and date_tag:\n","    #         title = title_tag.get_text(strip=True)\n","    #         date = date_tag.get(\"datetime\", \"\").split(\"T\")[0]  # Extract date from datetime attribute\n","\n","    #         # Filter titles based on criteria\n","    #         if \"coronavirus\" in title.lower() and not any(term in title.lower() for term in [\"daily briefing\", \"podcast\", \"backchat\"]):\n","    #             filtered_articles.append((date, title))\n","\n","    # # Sort first by date, then alphabetically by title\n","    # sorted_articles = sorted(filtered_articles, key=lambda x: (x[0], x[1]))\n","\n","    # # Return only the titles\n","    # return [title for _, title in sorted_articles]\n","###################\n","    search_url = f\"https://www.nature.com/search?q=coronavirus&article_type=news&journal=nature&date_range={start_date}%20to%20{end_date}\"\n","\n","    # Request the search page\n","    response = requests.get(search_url)\n","    if response.status_code != 200:\n","        raise Exception(f\"Failed to fetch search results. Status Code: {response.status_code}\")\n","\n","    # Parse the search results page\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    articles = soup.find_all(\"article\")\n","\n","    # Extract titles and dates\n","    filtered_articles = []\n","    for article in articles:\n","        title_tag = article.find(\"h2\")\n","        date_tag = article.find(\"time\")\n","\n","        if title_tag and date_tag:\n","            title = title_tag.get_text(strip=True)\n","            date = date_tag.get(\"datetime\", \"\").split(\"T\")[0]  # Extract date in YYYY-MM-DD format\n","\n","            # Apply filtering conditions\n","            if \"coronavirus\" in title.lower() and not any(term in title.lower() for term in [\"daily briefing\", \"podcast\", \"backchat\"]):\n","                filtered_articles.append((date, title))\n","\n","    # Sort first by date, then alphabetically by title\n","    sorted_articles = sorted(filtered_articles, key=lambda x: (x[0], x[1]))\n","\n","    # Return only the titles\n","    return [title for _, title in sorted_articles]\n","\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"_WqZAdaLjlgt","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"6f5717eb-a602-4fb9-ba39-d1dcdc6a759f"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-cd7360c7b3de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtest_extract_nature_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-cd7360c7b3de>\u001b[0m in \u001b[0;36mtest_extract_nature_articles\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_titles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mparsed_titles\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_titles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["def test_extract_nature_articles():\n","    parsed_titles = extract_nature_articles(\"2020-02-01\", \"2020-03-01\")\n","    expected_titles = [\n","        'HIV vaccine failure, coronavirus papers and an unprecedented glimpse of the Sun',\n","        'Did pangolins spread the China coronavirus to people?',\n","        'How scientists are fighting the novel coronavirus: A three minute guide',\n","        'CRISPR enhancement, coronavirus source and a controversial appointment',\n","        'Scientists fear coronavirus spread in countries least able to contain it',\n","        'More than 80 clinical trials launch to test coronavirus treatments',\n","        'When will the coronavirus outbreak peak?',\n","        'Coronavirus name, animal-research data and a Solar System snowman',\n","        'Scientists question China’s decision not to report symptom-free coronavirus cases',\n","        'China set to clamp down permanently on wildlife trade in wake of coronavirus',\n","        '‘No one is allowed to go out’: your stories from the coronavirus outbreak',\n","        'Time to use the p-word? Coronavirus enters dangerous new phase',\n","        'Mystery deepens over animal source of coronavirus'\n","    ]\n","    expected_dates = [\n","        '2020-02-05', '2020-02-07', '2020-02-07', '2020-02-12',\n","        '2020-02-13', '2020-02-15', '2020-02-18', '2020-02-19',\n","        '2020-02-20', '2020-02-21', '2020-02-21', '2020-02-25', '2020-02-26'\n","    ]\n","\n","    assert len(parsed_titles) == 13\n","    assert parsed_titles == expected_titles, parsed_titles\n","    print(\"All tests passed!\")\n","\n","test_extract_nature_articles()"]},{"cell_type":"markdown","metadata":{"id":"g1c-o8Y7jlgt"},"source":["To reduce bias in our data, it is typically a good idea to collect data from more than one source. Below are two **optional** challenges that involve the same web scraping process but on a different news site -- New York Times. Because it has a completely distinct HTML structure from Nature, you will need to inspect the webpage to identify the tags that contain the required information. The second optional challenge (Question 8) will also involve using Selenium to dynamically collect the search result. If you like to test your web scraping skill, give these questions a try."]},{"cell_type":"markdown","metadata":{"id":"TuNsxEEbjlgt"},"source":["### Parsing a single article from the New York Times (Optional)\n","\n"," **This is an optional, ungraded question. Feel free to proceed to Question 9 and revisit this later if you are interested.**\n","\n","Implement the function `parse_page_nyt` that takes as input an NYT article string URL and returns a JSON dictionary with the following format:\n","\n","```python\n","{\n","    'Title': 'F.D.A. Approves First Coronavirus Antibody Test in U.S.' #str\n","    'Author': ['Katie Thomas', 'Natasha Singer'] # list, a list of author names in the same order as they appear on the page\n","    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n","    'Summary': '.....' #str, the summary paragraph between the title and author fields, or empty string if no summary is available\n","    'Content': '.....' #list, the whole article content, where every element is a paragraph (i.e., comes from a <p> tag)\n","}\n","```\n","\n","The values of `Summary` and `Content` should be raw text that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b> <a href=\"https://google.com\">World</a><p>\"` then the output should be `\"Hello World\"`.\n","\n","In the local test we have provided the full reference JSON files for some article pages. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n","\n","**Notes**:\n","* Ignore the \"Frequently Asked Questions and Advice\" box (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nyt_faq.png)).\n","* The summary can be extracted from a `<p>` tag whose id is `article-summary`. If this tag is not present, simply use the empty string as the value.\n","* For the `Content` field, you only need to find the `<p>` tags inside the `<div>` tags that have the class `StoryBodyCompanionColumn`.\n","* You may find unicode characters, e.g., `\\u201d`, while parsing the page. It is fine to include them in the output.\n","* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1QOz0Fqjlgt"},"outputs":[],"source":["def parse_page_nyt(url):\n","    \"\"\"\n","    Parse a New York Times article page to extract the title, authors, date, summary and content.\n","\n","    args:\n","        url (str) : the article page's URL\n","\n","    return:\n","        Dict[str, str] : a JSON-like dictionary whose keys are \"Title\", \"Author\", \"Published Date\", \"Summary\" and \"Content\"\n","    \"\"\"\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"4p8sc77Jjlgt"},"outputs":[],"source":["def test_parse_page_nyt():\n","    nyt1 = parse_page_nyt(\"https://web.archive.org/web/20200603034948/https://www.nytimes.com/2020/04/21/health/fda-in-home-test-coronavirus.html\")\n","    nyt1_reference = json.load(open(\"local_test_refs/nyt1.txt\"))\n","    assert nyt1 == nyt1_reference\n","\n","    nyt2 = parse_page_nyt(\"https://web.archive.org/web/20200602022004/https://www.nytimes.com/2020/04/18/health/kidney-dialysis-coronavirus.html\")\n","    nyt2_reference = json.load(open(\"local_test_refs/nyt2.txt\"))\n","    assert nyt2 == nyt2_reference\n","\n","    nyt3 = parse_page_nyt(\"https://web.archive.org/web/20200520224040/https://www.nytimes.com/2020/03/23/health/medical-students-coronavirus.html\")\n","    nyt3_reference = json.load(open(\"local_test_refs/nyt3.txt\"))\n","    assert nyt3 == nyt3_reference\n","\n","    nyt4 = parse_page_nyt(\"https://web.archive.org/web/20200626171621/https://www.nytimes.com/2020/03/23/health/medical-students-coronavirus.html\")\n","    assert nyt4[\"Author\"] == ['Penina Krieger', 'Abby Goodnough']\n","    assert nyt4[\"Published Date\"] == \"2020-03-23\"\n","    assert len(nyt4[\"Content\"]) == 27\n","\n","    nyt5 = parse_page_nyt(\"https://web.archive.org/web/20200809005447/https://www.nytimes.com/2020/03/20/health/coronavirus-data-logarithm-chart.html\")\n","    assert nyt5[\"Author\"] == [\"Kenneth Chang\"]\n","    assert nyt5[\"Published Date\"] == \"2020-03-20\"\n","    assert len(nyt5[\"Content\"]) == 8\n","\n","    nyt6 = parse_page_nyt(\"https://web.archive.org/web/20200710083045/https://www.nytimes.com/2020/03/20/health/coronavirus-italy-men-risk.html\")\n","    assert nyt6[\"Author\"] == ['Roni Caryn Rabin']\n","    assert nyt6[\"Published Date\"] == \"2020-03-20\"\n","    assert len(nyt6[\"Content\"]) == 17\n","\n","    nyt7 = parse_page_nyt(\"https://web.archive.org/web/20200522153110/https://www.nytimes.com/2020/03/20/health/coronavirus-nurses-healthcare.html\")\n","    assert nyt7[\"Author\"] == ['Emma Goldberg']\n","    assert nyt7[\"Published Date\"] == \"2020-03-20\"\n","    assert len(nyt7[\"Content\"]) == 20\n","\n","    print(\"All tests passed!\")\n","\n","test_parse_page_nyt()"]},{"cell_type":"markdown","metadata":{"id":"yhsPy4Thjlgt"},"source":["### Parsing several NYT article titles from the search page (optional)\n","\n"," **This is an optional, ungraded question. Feel free to proceed to Question 9 and revisit this later if you are interested.**\n","\n","Just like with Nature, we can browse the [NYT search page](https://www.nytimes.com/search) and programmatically collect all the search results, which can then be parsed individually using `parse_page_nyt`.\n","\n","A caveat here is that the Nature search page partitions the search results into several webpages, each with a unique URL, while NYT includes all the search results in one webpage. Initially only the first 10 results are visible; we need to click on the \"SHOW MORE\" button to see the next 10 results, then click again for the next 10, and so on. Naturally, this task is best suited for Selenium.\n","\n","We provide the Selenium code to extract the search results here. This code will search for articles that:\n","\n","1. are presented by entering the term \"coronavirus\" in the search box.\n","1. were published in a given period of time (e.g., February 01, 2020 to March 01, 2020 inclusive)\n","1. have \"Article\" as the type and belong to the \"Health\" section  -- these criteria can be specified in the search result page.\n","\n","\n","To run this code you will need to have downloaded either the Chromedriver or Geckodriver, depending on your preferred browser (see the [Data Collection and Extraction Primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/data-collection-extraction-primer/data_collection_extraction_primer.ipynb) for detailed installation instructions). Then, edit the `USE_GECKODRIVER` variable, and fill in either the `PATH_TO_CHROMEDRIVER` or `PATH_TO_GECKODRIVER` variable below, then execute the code cell."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"Zgt-64C8jlgt"},"outputs":[],"source":["# set this to True if you are on Colab or using a Chromium browser, such as Google Chrome\n","# set this to False if you are using Firefox locally\n","USE_CHROMEDRIVER = True\n","\n","# path to the chromedriver executable, edit this if you use chromedriver locally\n","# if you are on Colab, do not change this path\n","# if you are on Windows, you may need to add .exe to the end of the path\n","PATH_TO_CHROMEDRIVER = \"./chromedriver\"\n","\n","# path to the geckodriver executable, edit this if you use geckodriver locally\n","# if you are on Windows, you may need to add .exe to the end of the path\n","PATH_TO_GECKODRIVER = \"./geckodriver\"\n","\n","# do not modify this function\n","def init_geckodriver(debug = False):\n","    options = webdriver.FirefoxOptions()\n","    if not debug:\n","        options.add_argument('--headless')\n","        options.add_argument('--no-sandbox')\n","        options.add_argument(\"--disable-setuid-sandbox\")\n","        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n","    return webdriver.Firefox(executable_path = PATH_TO_GECKODRIVER, options = options)\n","\n","# do not modify this function\n","def init_chromedriver(debug = False):\n","    options = webdriver.ChromeOptions()\n","    if not debug:\n","        options.add_argument('--headless')\n","        options.add_argument('--no-sandbox')\n","        options.add_argument(\"--disable-setuid-sandbox\")\n","        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n","    return webdriver.Chrome(executable_path = PATH_TO_CHROMEDRIVER, options = options)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8DD4B8Mjlgu"},"outputs":[],"source":["def extract_nyt_article_titles(start_date, end_date, base_url = \"https://www.nytimes.com\"):\n","    \"\"\"\n","    Search for and parse all coronavirus-related News article from the New York Times that were\n","    published in a given period\n","\n","    args:\n","        start_date (str): the lower bound of the date range to filter articles,\n","            has the format yyyy-mm-dd\n","        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n","            has the format yyyy-mm-dd\n","\n","    kwargs:\n","        base_url (str): the home page url of New York Times\n","\n","    return:\n","        List[Dict[str, str]] : a list of parsed JSON for each articles returned by\n","            the search query\n","    \"\"\"\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"usALjyVCjlgu"},"outputs":[],"source":["# print all the search result titles from the following page:\n","# https://www.nytimes.com/search?dropmab=true&endDate=20210301&query=coronavirus&sections=Health%7Cnyt%3A%2F%2Fsection%2F9f943015-a899-5505-8730-6d30ed861520&sort=oldest&startDate=20210201&types=article\n","extract_nyt_article_titles(\"2021-02-01\", \"2021-03-01\")"]},{"cell_type":"markdown","metadata":{"id":"ZIxuoWhwjlgu"},"source":["### Process news articles data\n","While the JSON data format we constructed earlier is useful for checking the correctness of our parsing, eventually we would like each article to be represented by just a string, one that we can input to `preprocess_text` and get a list of processed tokens. For our purpose, we will define the string representation of an article as\n","\n","`\"<title> <summary> <content paragraph 1> <content paragraph 2> <content paragraph 3> ...\"`\n","\n","where there is a single space separating each field (note that the content paragraphs come from the `\"Content\"` field of an article json, which is a list of paragraph strings).\n","\n","Implement the function `process_news_article` that takes as input a JSON dictionary resulting from parsing a Nature or NYT article, converts the JSON to the above string format, and outputs a list of processed tokens from the article string.\n","\n","**Note**:\n","* Remember to specify the stopwords parameter as `english_stopwords` when you call `preprocess_text`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tePvq_jrjlgu"},"outputs":[],"source":["def process_news_article(article_json):\n","    \"\"\"\n","    Convert article jsons to nested list of tokens of processed article contents\n","\n","    args:\n","        article_json (Dict[str, str]] : JSON content of a news article\n","\n","    return:\n","        List[str] : a list of processed tokens from the input article JSON\n","    \"\"\"\n","    title = article_json.get(\"Title\", \"\").strip()\n","    summary = article_json.get(\"Summary\", \"\").strip()\n","    content = \" \".join([para.strip() for para in article_json.get(\"Content\", [])])\n","\n","\n","    full_text = f\"{title} {summary} {content}\".strip()\n","\n","\n","    processed_tokens = preprocess_text(full_text, english_stopwords)\n","\n","    return processed_tokens\n","    pass\n","\n","# do not modify this function\n","def process_news_articles_data(article_jsons):\n","    return [process_news_article(article) for article in article_jsons]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"X8dZjCq9jlgu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dea10e04-8b33-4533-8296-cd6e2f1d110d"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_process_news_article():\n","    nature_article = json.load(open(\"local_test_refs/nature1.txt\"))\n","    nature_article_processed = process_news_article(nature_article)\n","    nature_expected = open(\"local_test_refs/nature1_processed.txt\").read().splitlines()\n","    assert nature_article_processed == nature_expected\n","\n","    nyt_article = json.load(open(\"local_test_refs/nyt1.txt\"))\n","    nyt_article_processed = process_news_article(nyt_article)\n","    nyt_expected = open(\"local_test_refs/nyt1_processed.txt\").read().splitlines()\n","    assert nyt_article_processed == nyt_expected\n","    print(\"All tests passed!\")\n","\n","test_process_news_article()"]},{"cell_type":"markdown","metadata":{"id":"3cjvutlojlgu"},"source":["##Mining PDF Data\n","Having extracted data from Twitter and newspapers, we now turn to our third source: research papers. We have provided you with 15 pdf files, collected from the [arxiv API](https://arxiv.org/help/api). These are located in the `pdfs` directory and labeled from `arxiv_01.pdf` to `arxiv_15.pdf`."]},{"cell_type":"markdown","metadata":{"id":"tDzlUtJLjlgu"},"source":["### Parse a single Arxiv research paper\n","Implement the function `parse_pdf` that takes as input a PDF file path and outputs the processed tokenization of the text content of that file. In particular, you should perform the following steps:\n","\n","1. Remove all URLs, i.e., strings that start with \"http://\" or \"https://\"\n","1. Call the `preprocess_text` function you implemented in part A. Remember to specify the `stopwords` parameter.\n","\n","**Notes**:\n","* For this question, you should use the function [`extract_text`](https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#extract-text) from the `pdfminer` package to convert a pdf file to string.\n","* Unlike in the tweet scenario, there is no limit on the length of an URL in this case. The URL pattern you should use here is: a string that starts with `http://` or `https://`, followed by any number of non-space character. Do not make any other assumption (for example, don't assume an URL always contains a `.`).\n","* We have provided a template helper function `remove_url_regex`, where you can enter the regex for removing URLs. There are some local test cases in `test_remove_url_regex` to help you validate your regex. If your regex passes these tests, you can use it in `parse_and_clean_pdf`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnDJpFJrjlgu"},"outputs":[],"source":["def remove_url_regex():\n","    # enter your regex for capturing url strings here\n","    # you can call this function in parse_and_clean_pdf()\n","    # regex = r\"http[s]?://\\S+\"\n","\n","    regex = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n","    return regex\n","\n","def parse_and_clean_pdf(file):\n","    \"\"\"\n","    Convert an input pdf file into processed and cleaned raw text.\n","\n","    args:\n","        file (str) : the pdf file path\n","\n","    return:\n","        List[str] : the cleaned tokenization of the input file content\n","    \"\"\"\n","\n","    raw_text = high_level.extract_text(file)\n","    if not raw_text.strip():\n","        raise ValueError(\"PDF extraction failed: No text was extracted.\")\n","\n","    url_pattern = remove_url_regex()\n","    cleaned_text = re.sub(url_pattern, \"\", raw_text)\n","\n","    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n","\n","    processed_text = preprocess_text(cleaned_text, english_stopwords)\n","\n","    return processed_text\n","\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"05D3Tr4Djlgu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f3921ab-ed22-4911-e8d9-ecc50f2fb1fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_remove_url_regex():\n","    s = \"http://abc\"\n","    assert re.sub(remove_url_regex(), '', s) == ''\n","\n","    s = 'hellohttps://github.com/lanagarmire/COVID19-Drugs-LungInjury'\n","    assert re.sub(remove_url_regex(), '', s) == 'hello'\n","\n","    s = 'http://example.com https://cmu.edu'\n","    assert re.sub(remove_url_regex(), '', s) == ' '\n","\n","    s = 'https://www.'\n","    assert re.sub(remove_url_regex(), '', s) == ''\n","    print(\"All tests passed!\")\n","\n","test_remove_url_regex()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"BHoJmARQjlgu","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"0c86927e-1794-4e10-bae6-b697d72c55aa"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-551b00d4b0c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_parse_and_clean_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-551b00d4b0c7>\u001b[0m in \u001b[0;36mtest_parse_and_clean_pdf\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_and_clean_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pdfs/arxiv_01.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local_test_refs/parsed_arxiv_01.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mpdf_text\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["def test_parse_and_clean_pdf():\n","    pdf_text = parse_and_clean_pdf(\"pdfs/arxiv_01.pdf\")\n","    with open(\"local_test_refs/parsed_arxiv_01.txt\") as outfile:\n","        assert pdf_text == outfile.read().splitlines()\n","    print(\"All tests passed!\")\n","\n","test_parse_and_clean_pdf()"]},{"cell_type":"markdown","metadata":{"id":"KF74DeOsjlgu"},"source":["### Parse several Arxiv research papers\n","Implement the function `process_arxiv_data` that takes as input the path to a directory. This function parses and cleans all pdf files in that directory, then returns a nested list of word tokens, where each inner list results from parsing one PDF file.\n","\n","**Notes**:\n","* The pdf files should be processed based on the alphabetical order of their name, e.g., `arxiv_01.pdf` before `arxiv_02.pdf`.\n","* Do not assume that `os.listdir` will return the filenames in sorted order; you should perform the sorting yourself.\n","* Do not assume every file in the input directory is a pdf file; only those whose names end in `.pdf` should be parsed.\n","* If you fail the test case here, it is likely that your URL removal regex from Question 10 is incorrect. Try to come up with more test cases to test your URL."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKzUqYkBjlgu"},"outputs":[],"source":["def process_arxiv_data(directory):\n","    \"\"\"\n","    Parse and process the text content of all pdf papers in alphabetical order in a given directory\n","\n","    args:\n","        directory (str) : the relative file path to a directory that contains the pdf papers\n","\n","    return:\n","        List[List[str]] : a list of list of word tokens\n","    \"\"\"\n","    pdf_dir = \"pdfs/\"\n","    pdf_files = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n","\n","    all_papers_tokens = []\n","    for pdf_file in pdf_files:\n","        file_path = os.path.join(pdf_dir, pdf_file)\n","        paper_tokens = parse_and_clean_pdf(file_path)\n","        all_papers_tokens.append(paper_tokens)\n","\n","    return all_papers_tokens\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"d8rpBgQWjlgu","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"a89e1e88-06b9-4f4b-ada0-037c6423b476"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-26aa839d9321>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtest_process_arxiv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-26aa839d9321>\u001b[0m in \u001b[0;36mtest_process_arxiv_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_contents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     assert first_words == [\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34m'repurposed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fractal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coronavirus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xu1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parametric'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'view'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'insight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reconstruction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'covid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outbreak'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["def test_process_arxiv_data():\n","    paper_contents = process_arxiv_data(\"pdfs\")\n","    first_words = [paper[0] for paper in paper_contents]\n","    mid_words = [paper[100] for paper in paper_contents]\n","    last_words = [paper[-1] for paper in paper_contents]\n","\n","    assert len(paper_contents) == 15\n","    assert first_words == [\n","        'repurposed', 'fractal', 'coronavirus', 'data', 'xu1', 'parametric',\n","        'view', 'insight', 'reconstruction', 'covid', 'outbreak', 'scale',\n","        'abnormal', 'trend', 'deep'\n","    ]\n","    assert mid_words == [\n","        'result', 'nal', 'ro', 'february', 'covid', 'sars',\n","        'virus', 'export', 'system', 'new', 'transmission', 'cantly',\n","        'signi', 'content', 'wenling'\n","    ]\n","    assert last_words == [\n","        '2019', '13', '122567', '13', 'lancet', 'url',\n","        '77', 'hour', '330', '16', 'provide', '2004',\n","        '212', '2020', '29'\n","    ]\n","    assert sum(len(paper) for paper in paper_contents) == 35969\n","    print(\"All tests passed!\")\n","\n","test_process_arxiv_data()"]},{"cell_type":"markdown","metadata":{"id":"Wvm8RQdKjlgu"},"source":["## Data Visualization and Feature Construction\n","Now that we have collected text data from three different sources (Twitter, news articles and research papers), let's put them all together in order to perform some simple exploratory data analyses and feature construction. From now we will define a *document* as a list of tokens coming from a single tweet, news article or arxiv paper, and a *corpus* as a list of documents."]},{"cell_type":"markdown","metadata":{"id":"SGhtD-lDjlgv"},"source":["###Word frequency and word cloud\n","With any text corpus, you will first want to check for the word frequency distribution, in particular which words are the most common and which are the least. The former group may consist of terms that are relevant to the topic, or terms that simply appear frequently in general (e.g., stopwords). The latter group may consist of highly specialized terms or typos. Since stopwords and rare words are not useful to our analysis, we will remove both (where we define rare words as words that only appear *once in the corpus*).\n","\n","Implement the function `word_frequency` which takes as input a text corpus and returns a `collections.Counter` object mapping each word to its frequency in the corpus. However, rare words that only appear once in the entire corpus should **not** be included in this mapping.\n","\n","**Notes**:\n","* Recall that `preprocess_text` already handles stopword removal, so you only need to remove rare words in this step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-BBjvPijlgv"},"outputs":[],"source":["def word_frequency(corpus):\n","    \"\"\"\n","    Count the word frequency in a given corpus\n","\n","    args:\n","        corpus (List[List[str]]) : a nested list of tokens, where each inner list is a processed document\n","\n","    return:\n","        collections.Counter : a mapping between each word and its frequency in the corpus, excluding words that\n","            only appear once\n","    \"\"\"\n","    all_words = [word for doc in corpus for word in doc]\n","\n","\n","    word_counts = collections.Counter(all_words)\n","\n","\n","    filtered_counts = collections.Counter({word: count for word, count in word_counts.items() if count > 1})\n","\n","    return filtered_counts\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"pTIYzROGjlgv","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"d28ec7aa-76b4-4fa0-d47b-6f4325961f22"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-0edbe7c6a113>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtest_word_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-0edbe7c6a113>\u001b[0m in \u001b[0;36mtest_word_frequency\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1739\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coronavirus\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m407\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coronavirusoutbreak\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m684\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["def test_word_frequency():\n","    tweet_corpus = process_tweet_data(\n","        get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\", n_tweets = 1000)\n","    )\n","    counter = word_frequency(tweet_corpus)\n","    assert len(counter) == 1739\n","    assert counter[\"coronavirus\"] == 407\n","    assert counter[\"coronavirusoutbreak\"] == 684\n","    assert counter[\"increasingly\"] == 2\n","    assert counter[\"stimulus\"] == 3\n","    assert min(counter.values()) == 2\n","    print(\"All tests passed!\")\n","\n","test_word_frequency()"]},{"cell_type":"markdown","metadata":{"id":"XngcNqdrjlgv"},"source":["Now we will gather all three corpuses together; we store them in a global cache to avoid having to construct them more than once. If you make any code change above this point, rerun the following cell to reset the cache."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5ZgclK9jlgv"},"outputs":[],"source":["corpuses = None\n","\n","def get_corpuses():\n","    global corpuses\n","    if corpuses is None:\n","        twitter_corpus = process_tweet_data(\n","            get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\")\n","        )\n","        news_data_files = [\n","            \"nature1.txt\", \"nature2.txt\", \"nature3.txt\",\n","            \"nyt1.txt\", \"nyt2.txt\", \"nyt3.txt\"\n","        ]\n","        news_corpus = process_news_articles_data([\n","            json.load(open(f\"local_test_refs/{filename}\"))\n","            for filename in news_data_files\n","        ])\n","        arxiv_corpus = process_arxiv_data(\"pdfs\")\n","        corpuses = (twitter_corpus, news_corpus, arxiv_corpus)\n","    return corpuses"]},{"cell_type":"markdown","metadata":{"id":"kqHzNX1Zjlgv"},"source":["Let's first compare the frequency of a number of keywords across these three corpuses."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"WCgbG3prjlgv","colab":{"base_uri":"https://localhost:8080/","height":892},"outputId":"f8ce61b2-fc9d-4bed-f643-f2cb2f9331cc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["             Proportion in twitter corpus  Proportion in news corpus  \\\n","coronavirus                      0.027503                   0.007628   \n","covid                            0.011583                   0.003337   \n","case                             0.007142                   0.004291   \n","health                           0.002728                   0.004291   \n","model                            0.000128                   0.001192   \n","say                              0.003699                   0.022646   \n","test                             0.004370                   0.010727   \n","2020                             0.000946                   0.000000   \n","19                               0.009410                   0.003099   \n","people                           0.006612                   0.009058   \n","vaccine                          0.000562                   0.008820   \n","\n","             Proportion in arxiv corpus  \n","coronavirus                    0.004059  \n","covid                          0.007283  \n","case                           0.007923  \n","health                         0.001779  \n","model                          0.010424  \n","say                            0.000083  \n","test                           0.001084  \n","2020                           0.011342  \n","19                             0.008256  \n","people                         0.003447  \n","vaccine                        0.000056  "],"text/html":["\n","  <div id=\"df-61cbe913-c171-473b-ab86-c87d9b8dbbf2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Proportion in twitter corpus</th>\n","      <th>Proportion in news corpus</th>\n","      <th>Proportion in arxiv corpus</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>coronavirus</th>\n","      <td>0.027503</td>\n","      <td>0.007628</td>\n","      <td>0.004059</td>\n","    </tr>\n","    <tr>\n","      <th>covid</th>\n","      <td>0.011583</td>\n","      <td>0.003337</td>\n","      <td>0.007283</td>\n","    </tr>\n","    <tr>\n","      <th>case</th>\n","      <td>0.007142</td>\n","      <td>0.004291</td>\n","      <td>0.007923</td>\n","    </tr>\n","    <tr>\n","      <th>health</th>\n","      <td>0.002728</td>\n","      <td>0.004291</td>\n","      <td>0.001779</td>\n","    </tr>\n","    <tr>\n","      <th>model</th>\n","      <td>0.000128</td>\n","      <td>0.001192</td>\n","      <td>0.010424</td>\n","    </tr>\n","    <tr>\n","      <th>say</th>\n","      <td>0.003699</td>\n","      <td>0.022646</td>\n","      <td>0.000083</td>\n","    </tr>\n","    <tr>\n","      <th>test</th>\n","      <td>0.004370</td>\n","      <td>0.010727</td>\n","      <td>0.001084</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>0.000946</td>\n","      <td>0.000000</td>\n","      <td>0.011342</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.009410</td>\n","      <td>0.003099</td>\n","      <td>0.008256</td>\n","    </tr>\n","    <tr>\n","      <th>people</th>\n","      <td>0.006612</td>\n","      <td>0.009058</td>\n","      <td>0.003447</td>\n","    </tr>\n","    <tr>\n","      <th>vaccine</th>\n","      <td>0.000562</td>\n","      <td>0.008820</td>\n","      <td>0.000056</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61cbe913-c171-473b-ab86-c87d9b8dbbf2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-61cbe913-c171-473b-ab86-c87d9b8dbbf2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-61cbe913-c171-473b-ab86-c87d9b8dbbf2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bc613f66-6c9b-473f-8793-53b96fce8ac4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc613f66-6c9b-473f-8793-53b96fce8ac4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bc613f66-6c9b-473f-8793-53b96fce8ac4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_d9ae7d71-ba89-4798-9746-d08be2d5b414\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_frequency')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d9ae7d71-ba89-4798-9746-d08be2d5b414 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_frequency');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_frequency","summary":"{\n  \"name\": \"df_frequency\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"Proportion in twitter corpus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0077991226419480844,\n        \"min\": 0.00012777266687110294,\n        \"max\": 0.027503066544004907,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.00369901870591843,\n          0.027503066544004907,\n          0.006612235510579577\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proportion in news corpus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006263766811331153,\n        \"min\": 0.0,\n        \"max\": 0.02264600715137068,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.009058402860548272,\n          0.0033373063170441,\n          0.010727056019070322\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proportion in arxiv corpus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0041422023010933,\n        \"min\": 5.559725349567731e-05,\n        \"max\": 0.011341839713118172,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          8.339588024351597e-05,\n          0.004058599505184444,\n          0.0034470297167319935\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<Axes: >"]},"metadata":{},"execution_count":39},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# this cell has been taggged with excluded_from_script\n","# it will not be run by the autograder\n","def get_word_frequency_across_corpuses(input_words):\n","    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n","    twitter_corpus_size = sum(len(d) for d in twitter_corpus)\n","    news_corpus_size = sum(len(d) for d in news_corpus)\n","    arxiv_corpus_size = sum(len(d) for d in arxiv_corpus)\n","    twitter_f, news_f, arxiv_f = word_frequency(twitter_corpus), word_frequency(news_corpus), word_frequency(arxiv_corpus)\n","    return pd.DataFrame({\n","        \"Proportion in twitter corpus\" : [twitter_f.get(word, 0) / twitter_corpus_size for word in input_words],\n","        \"Proportion in news corpus\" : [news_f.get(word, 0) / news_corpus_size for word in input_words],\n","        \"Proportion in arxiv corpus\" : [arxiv_f.get(word, 0) / arxiv_corpus_size for word in input_words]\n","    }, index = input_words)\n","\n","df_frequency = get_word_frequency_across_corpuses([\n","    \"coronavirus\", \"covid\", \"case\", \"health\", \"model\", \"say\", \"test\",\n","    \"2020\", \"19\", \"people\", \"vaccine\"\n","])\n","\n","display(df_frequency)\n","\n","df_frequency.plot(kind='bar')"]},{"cell_type":"markdown","metadata":{"id":"zWjwsEo9jlgv"},"source":["We see that there are differences across datasets in the relative frequency of each term. \"Coronavirus\" is used most frequently in tweets, \"say\" most frequently in news corpus, and perhaps unsurprisingly, \"model\" most frequently in arxiv papers. The scientific notation of coronavirus, \"covid,\" isn't used in news articles as much, but is equally popular in both tweets and arxiv papers. On the other hand, \"health\" sees most frequent usage in news articles, likely due to health advice-related articles. Feel free to edit the word list above and see what other insights you can derive!"]},{"cell_type":"markdown","metadata":{"id":"eqVtQNhUjlgv"},"source":["We now move to the last step of data collection and preparation: constructing input features to be used for more formal analyses and language modeling. As language modeling will be covered later in the course, here we will only cover two simple feature construction methods: term frequency (TF) and term frequency - inverse document frequency (TF-IDF)."]},{"cell_type":"markdown","metadata":{"id":"AX5iCUF5jlgv"},"source":["### Feature construction: term frequency (TF)\n","Implement the function `construct_tf_matrix` that takes as input a corpus and outputs a matrix $TF$ where each row corresponds to one document, and each column corresponds to one of the unique words in the entire corpus. $TF_{ij}$ is the number of times word $j$ appears in document $i$. Similar to the previous question, rare words that only appear once in the entire corpus should be removed, i.e., there should be no columns for those words.\n","\n","**Notes**:\n","* The rows should be ordered based on the document ordering in the corpus. Row 0 corresponds to `corpus[0]`, row 1 to `corpus[1]`, and so on.\n","* The columns should be ordered based on the alphabetical order of their corresponding words. Column 0 corresponds to the alphabetically first word in the corpus, column 1 to the alphabetically second word, and so on.\n","* To ensure code efficiency, avoid using too many loops. Take advantage of Pandas and Numpy functionalities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7GSVGK2jlgv"},"outputs":[],"source":["def construct_tf_matrix(corpus):\n","    \"\"\"\n","    Construct a term frequency matrix from an input corpus\n","\n","    args:\n","        corpus (List[List[str]]) : a nested list of word tokens, where each inner list is a document\n","\n","    return:\n","        np.array[n_documents, n_words] : the term frequency matrix\n","    \"\"\"\n","    word_counts = word_frequency(corpus)\n","    valid_words = set(word_counts.keys())  # Only include words appearing more than once\n","\n","\n","    doc_word_counts = [{word: doc.count(word) for word in valid_words} for doc in corpus]\n","\n","\n","    tf_matrix = pd.DataFrame(doc_word_counts).fillna(0).astype(int)\n","\n","\n","    tf_matrix = tf_matrix[sorted(tf_matrix.columns)]\n","\n","    return tf_matrix.to_numpy()\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"Vigb-RdNjlgv","colab":{"base_uri":"https://localhost:8080/","height":341},"outputId":"236710c0-88cf-44c4-dc1b-924c850aec19"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"(10021, 9879)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-93ec4e8805b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_construct_tf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-41-93ec4e8805b3>\u001b[0m in \u001b[0;36mtest_construct_tf_matrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_tf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_corpuses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10021\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9871\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1465\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1935\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4681\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1857\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2093\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2496\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1993\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2324\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1809\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3369\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: (10021, 9879)"]}],"source":["def test_construct_tf_matrix():\n","    corpus = [\n","        \"this project is project 4 in foundations of computational data science\".split(),\n","        \"it covers text data collection and preparation in the data science pipeline\".split(),\n","        \"text processing can be tricky sometimes\".split()\n","    ]\n","    tf = construct_tf_matrix(corpus)\n","    assert (tf == np.array([\n","        [1, 1, 2, 1, 0],\n","        [2, 1, 0, 1, 1],\n","        [0, 0, 0, 0, 1]]\n","    )).all()\n","\n","    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n","    all_corpuses = twitter_corpus + news_corpus + arxiv_corpus\n","    tf = construct_tf_matrix(all_corpuses)\n","    assert tf.dtype == np.int64\n","    assert tf.shape == (10021, 9871), tf.shape\n","    assert (tf.sum(axis = 1)[:10] == np.array([9, 14, 10, 17, 27, 4, 22, 23, 17, 13])).all(), tf.sum(axis = 1)[:10]\n","    assert (tf.sum(axis = 1)[-10:] == np.array([1465, 1935, 4681, 1857, 2093, 2496, 1993, 2324, 1809, 3369])).all(), tf.sum(axis = 1)[-10:]\n","    print(\"All tests passed!\")\n","\n","test_construct_tf_matrix()"]},{"cell_type":"markdown","metadata":{"id":"EK7z5bS5jlgv"},"source":["### Feature construction: term frequency - inverse document frequency (TF-IDF)\n","We can now compute the TF-IDF matrix, which scales the columns of the term frequency matrix by their inverse document frequency. Recall that the inverse document frequency of a word $j$ is computed as\n","$$\\text{IDF}_j = \\log \\left( \\frac{\\# \\text{ of documents}}{\\# \\text{ of documents with word } j} \\right),$$\n","and so the $\\text{TF-IDF}_{ij}$ entry in the tf-idf matrix is computed as\n","$$\\text{TF-IDF}_{ij} = \\text{TF}_{ij} \\times \\text{IDF}_j.$$\n","\n","Implement the function `tf_idf_matrix` which takes as input a TF matrix and outputs the corresponding TF-IDF matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35wA6jwpjlgv"},"outputs":[],"source":["def construct_tf_idf_matrix(tf_matrix):\n","    \"\"\"\n","    Compute the term frequency - inverse document frequency in a corpus\n","\n","    args:\n","        tf_matrix (np.array[n_documents, n_words]) : the term frequency document of the corpus\n","\n","    return:\n","        np.array[n_documents, n_words] : the tf-idf matrix\n","    \"\"\"\n","    n_documents = tf_matrix.shape[0]\n","    n_words = tf_matrix.shape[1]\n","    idf = np.log(n_documents / np.count_nonzero(tf_matrix, axis=0))\n","    tf_idf = tf_matrix * idf\n","    return tf_idf\n","\n","\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["excluded_from_script"],"id":"Z5DsPCQrjlgv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"546887d2-a47d-4393-b67f-a34be0282dde"},"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}],"source":["def test_construct_tf_idf_matrix():\n","    corpus = [\n","        \"this project is project 4 in foundations of computational data science\".split(),\n","        \"it covers text data collection and preparation in the data science pipeline\".split(),\n","        \"text processing can be tricky sometimes\".split()\n","    ]\n","    tf_idf = construct_tf_idf_matrix(construct_tf_matrix(corpus))\n","    assert np.allclose(tf_idf, np.array([\n","        [0.40546511, 0.40546511, 2.19722458, 0.40546511, 0.        ],\n","        [0.81093022, 0.40546511, 0.        , 0.40546511, 0.40546511],\n","        [0.        , 0.        , 0.        , 0.        , 0.40546511]\n","    ]))\n","    print(\"All tests passed!\")\n","\n","test_construct_tf_idf_matrix()"]}],"metadata":{"celltoolbar":"Tags","kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"nteract":{"version":"nteract-front-end@1.0.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}